{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"RDD-application-1\") \\\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: <br>\n",
    "\n",
    "Nhớ rằng chỉ có 3 cách để tạo ra một RDD:\n",
    "- Load từ storage\n",
    "- Thực hiện từ spark driver (spark context)\n",
    "- Tạo từ một RDD khác: chính là các transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thực hiện đọc từ Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create RDD using sparkContext.textFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create RDD from external Data source\n",
    "rdd2 = spark.sparkContext.textFile(\"/path/textFile.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create RDD using sparkContext.wholeTextFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = spark.sparkContext.wholeTextFiles(\"/path/textFile.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thực hiện từ Spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create using sparkContext.parallelize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "rdd = spark.sparkContext.parallelize(data, numSlices=10)   # numSlices chính là số partition của RDD\n",
    "rdd2 = spark.sparkContext.parallelize(range(0, 6, 1), numSlices=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Image: RDD creation <br>\n",
    "![alt text](imgs/rdd-creation-1.png \"image title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create empty RDD using sparkContext.emptyRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates empty RDD with no partition\n",
    "rdd = spark.sparkContext.emptyRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD partition manupulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** <br>\n",
    "When we use **parallelize(no set partition)** or **textFile()** or **wholeTextFiles()** methods of SparkContext to initiate RDD <br>\n",
    "=> it automatically splits the data into partitions based on resource availability (number of cores available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.textFile(\"file:///home/hadoop/Codes/pyspark_projects/tmp_data/text.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial partition count:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"initial partition count: \", rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repartition() <br>\n",
    "- Thực chất là 1 dạng Wide Transform\n",
    "- Note that repartition() method is a very expensive operation as it shuffles data from all nodes in a cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re-partition count:  4\n"
     ]
    }
   ],
   "source": [
    "reparRdd = rdd.repartition(4)\n",
    "print(\"re-partition count: \", reparRdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coalesce()\n",
    "- Thực chất là 1 dạng Wide Transform\n",
    "- Chỉ định số nodes mà rdd sẽ di chuyển qua. Ví dụ coalesce(2) => các partition chỉ đi qua 2 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reparRdd = rdd.coalesce(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "- Transformation luôn trả về một rdd khác\n",
    "- Có hai loại transformation:\n",
    "    + Narrow Transformation: Mỗi partition đầu vào sẽ dùng để tính 1 partition đầu ra\n",
    "    + Wide Transformation (shuffle transformation): Nhiều partition đầu vào sẽ dùng để tính 1 partition đầu ra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrow Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatmap() <br>\n",
    "- flatMap() transformation flattens the RDD after applying the function in map and returns a new RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd.flatMap(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map()\n",
    "- the output of map transformations would always have the same number of records as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = rdd2.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduceByKey()\n",
    "- merges the values for each key with the function specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4 = rdd3.reduceByKey(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sortByKey() (của wide transform)\n",
    "- transformation is used to sort RDD elements on key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 'the'), (4, 'Lorem'), (4, 'of'), (3, 'Ipsum'), (3, 'and'), (2, 'dummy'), (2, 'text'), (2, 'has'), (2, 'a'), (2, 'type'), (2, 'It'), (2, 'with'), (1, 'is'), (1, 'simply'), (1, 'printing'), (1, 'typesetting'), (1, 'industry.'), (1, 'been'), (1, \"industry's\"), (1, 'standard'), (1, 'ever'), (1, 'since'), (1, '1500s,'), (1, 'when'), (1, 'an'), (1, 'unknown'), (1, 'printer'), (1, 'took'), (1, 'galley'), (1, 'scrambled'), (1, 'it'), (1, 'to'), (1, 'make'), (1, 'specimen'), (1, 'book.'), (1, 'survived'), (1, 'not'), (1, 'only'), (1, 'five'), (1, 'centuries,'), (1, 'but'), (1, 'also'), (1, 'leap'), (1, 'into'), (1, 'electronic'), (1, 'typesetting,'), (1, 'remaining'), (1, 'essentially'), (1, 'unchanged.'), (1, 'was'), (1, 'popularised'), (1, 'in'), (1, '1960s'), (1, 'release'), (1, 'Letraset'), (1, 'sheets'), (1, 'containing'), (1, 'passages,'), (1, 'more'), (1, 'recently'), (1, 'desktop'), (1, 'publishing'), (1, 'software'), (1, 'like'), (1, 'Aldus'), (1, 'PageMaker'), (1, 'including'), (1, 'versions'), (1, 'Ipsum.')]\n"
     ]
    }
   ],
   "source": [
    "rdd5 = rdd4.map(lambda x: (x[1],x[0])).sortByKey(ascending=False)\n",
    "#Print rdd5 result to console\n",
    "print(rdd5.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter() \n",
    "- transformation is used to filter the records in an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 'the'), (4, 'Lorem'), (4, 'of'), (3, 'Ipsum'), (3, 'and'), (2, 'dummy'), (2, 'text'), (2, 'has'), (2, 'a'), (2, 'type'), (2, 'It'), (2, 'with')]\n"
     ]
    }
   ],
   "source": [
    "rdd6 = rdd5.filter(lambda x : x[0] > 1)\n",
    "print(rdd6.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "union()\n",
    "- gom 2 rdd lại với nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_rdd = rdd2.union(rdd3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample()\n",
    "- Lấy sample dữ liệu\n",
    "    + nếu không có withReplacement => Fraction là tỉ lệ % đầu ra [0, 1]\n",
    "    + nếu có withReplacement => Fraction là số lần xuất hiện (kì vọng) của mỗi phần tử được chọn\n",
    "- withReplacement: cho phép hoàn trả hay không"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 5, 6, 9]\n",
      "[1, 1, 1, 2, 2, 3, 3, 4, 4, 4, 4, 4, 5, 6, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize(range(1, 10))\n",
    "sample_rdd = rdd.sample(withReplacement=False, fraction=0.7)  \n",
    "sample_rdd_2 = rdd.sample(True, fraction=1)\n",
    "print(sample_rdd.collect())\n",
    "print(sample_rdd_2.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**<br>\n",
    "- Thường có dạng <key, value>: Pair RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "groupByKey() and reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('one', 1), ('two', 2), ('three', 3)]\n",
      "[('one', 1), ('two', 2), ('three', 3)]\n"
     ]
    }
   ],
   "source": [
    "words = [\"one\", \"two\", \"two\", \"three\", \"three\", \"three\"]\n",
    "wordPairsRDD = spark.sparkContext.parallelize(words).map(lambda word : (word, 1))\n",
    "\n",
    "wordCountsWithReduce = wordPairsRDD.reduceByKey(lambda x, y: x + y).collect()\n",
    "wordCountsWithGroup= wordPairsRDD.groupByKey().map(lambda x: (x[0], sum(x[1]))).collect()\n",
    "\n",
    "print(wordCountsWithGroup)\n",
    "print(wordCountsWithReduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image: reduceByKey vs groupByKey <br>\n",
    "![](imgs/ReduceAndGroupByKey.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join() - distinct() - intersect()\n",
    "- join 2 rdd dựa trên key\n",
    "- distinct lấy ra các phần tử không bị trùng lặp\n",
    "- intersect lấy phần giao nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data join: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(2, ('b', 'e'))\n",
      "(1, ('a', 'a'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data intersect\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(1, 'a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data distinct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(1, 'a')\n",
      "(2, 'b')\n",
      "(3, 'c')\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.parallelize([(1, 'a'), (2, 'b'), (3, 'c')])\n",
    "rdd2 = spark.sparkContext.parallelize([(1, 'a'), (2, 'e'), (4, 'f')])\n",
    "\n",
    "print(\"data join: \")\n",
    "data_join = rdd1.join(rdd2)\n",
    "data_join.foreach(print)\n",
    "\n",
    "print(\"data intersect\")\n",
    "data_intersect = rdd1.intersection(rdd2)\n",
    "data_intersect.foreach(print)\n",
    "\n",
    "print(\"data distinct\")\n",
    "data_distinct = rdd1.distinct()\n",
    "data_distinct.foreach(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "- Action luôn trả về giá trị hoặc list giá trị thay vì trả về một rdd mới\n",
    "- Nên hiểu reduceByKey và reduce nói chung là sẽ đưa hết các record về cùng 1 thằng\n",
    "    + Nếu không có ByKey thì reduce tất cả về 1 record duy nhất\n",
    "    + Nếu có ByKey thì reduce tất cả những thằng cùng key về với nhau \n",
    "    (key ở đây có thể là phần tử đầu tiên hoặc function mà mình tự định nghĩa)\n",
    "    + Hàm lambda truyền vào cho thằng reduce gồm hai biến tương trưng cho 2 thằng cùng được reduce liên tiếp, áp dụng lần lượt từng cặp một cho đến khi về 1 record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- count() – Returns the number of records in an RDD <br>\n",
    "- first() – Returns the first record. <br>\n",
    "- max() – Returns max record. <br>\n",
    "- reduce() – Reduces the records to single, we can use this to count or sum. <br>\n",
    "- take() – Returns the record specified as an argument, trả về một rdd mới với số lượng các records theo argument <br>\n",
    "- collect() – Returns all data from RDD as an array; maybe out of mem if the rdd is huge <br>\n",
    "- saveAsTextFile() – write the RDD to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 7\n",
      "First element: (1, 'a')\n",
      "Max element by key: (11, 'e')\n",
      "Max element by value: (5, 'f')\n",
      "Reduce:  (29, 'abcdfec')\n",
      "3 records:  [(1, 'a'), (2, 'b'), (3, 'c')]\n",
      "[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd'), (5, 'f'), (11, 'e'), (3, 'c')]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd'), (5, 'f'), (11, 'e'), (3, 'c')])\n",
    "\n",
    "print(f\"Count: {rdd.count()}\")\n",
    "print(f\"First element: {rdd.first()}\")\n",
    "print(f\"Max element by key: {rdd.max()}\")\n",
    "print(f\"Max element by value: {rdd.max(key=(lambda x: x[1]))}\")\n",
    "print(f\"Reduce: \", rdd.reduce(lambda x, y: (x[0] + y[0], x[1] + y[1])))\n",
    "print(f\"3 records: \", rdd.take(3))\n",
    "print(rdd.collect())\n",
    "rdd.saveAsTextFile(\"file:///home/hadoop/Codes/pyspark_projects/tmp_data/text_from_rdd.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** <br>\n",
    "Mục đích:\n",
    "- Nhằm tận dụng lại những computation đã thực hiện. \n",
    "\n",
    "Động lực:\n",
    "- Về bản chất thì Spark không hề lưu các giá trị hay các rdd trung gian được tính ra trong các bước transformation, nó chỉ lưu linage graph - đồ thị diễn đạt các bước thực hiện transform này mà thôi.\n",
    "- Chúng chỉ thực hiện transform (lazy operation) và lưu tạm thời khi có một action nào đó được kêu gọi.\n",
    "\n",
    "Vấn đề: Đặt ra vấn đề là nếu muốn sử dụng lại một rdd trung gian nào đó thì phải làm thế nào?\n",
    "- Thực hiện tính toán lại từ đầu ? => Rất tốn chi phí\n",
    "=> Do đó Spark cho phép lưu tạm thời (caching) thằng rdd xuống memory/disk để thực hiện các subsequent transformation/actions khác\n",
    "=> Lúc này mỗi node sẽ lưu các partitions của cached rdd mà node đó đang nắm giữ xuống memory/disk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cache():\n",
    "- Sẽ tự động gọi đến persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedRdd = rdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "persist():\n",
    "- persist cho phép thiết lập StorageLevel:\n",
    "    + MEMORY_ONLY : default, lưu xuống JVM memory nhưng không tuần tự hóa\n",
    "    + MEMORY_ONLY_SER : lưu xuống memory nhưng tuần tự hóa, tiết kiệm space hơn với MEMORY_ONLY, nhưng mất thêm vài CPU cycles để giải tuần tự hóa\n",
    "    + MEMORY_AND_DISK : lưu xuống memory nhưng không tuần tự hóa, nếu RDD cần lưu xuống lớn hơn RAM hiện có => lưu các excess partitions xuống disk \n",
    "    + MEMORY_AND_DISK_SER : như MEMORY_AND_DISK nhưng tuần tự\n",
    "    + DISK_ONLY\n",
    "    <br><br>\n",
    "    + MEMORY_ONLY_2 : tương tự nhưng replication từng partition ra 2 nodes\n",
    "    + MEMORY_ONLY_SER_2 : tương tự nhưng replication từng partition ra 2 nodes\n",
    "    + MEMORY_AND_DISK_2 : tương tự nhưng replication từng partition ra 2 nodes\n",
    "    + MEMORY_AND_DISK_SER_2 : tương tự nhưng replication từng partition ra 2 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPersist = rdd.persist(pyspark.StorageLevel.MEMORY_ONLY) \n",
    "dfPersist.show(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unpersist()\n",
    "- PySpark drops persisted data if not used or by using least-recently-used (LRU) algorithm.\n",
    "- Can also manually remove using unpersist() method. unpersist() marks the RDD as non-persistent, and remove all blocks for it from memory and disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
