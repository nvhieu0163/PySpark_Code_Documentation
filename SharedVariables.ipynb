{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"Shared-Variables-application-1\") \\\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Động lực\n",
    "\n",
    "- Khi thực hiện các transformation như reduce() hay map() thì spark sẽ thực hiện transform trên các remote node (slave nodes). Các variables sẽ được gửi đi cùng với tasks và không quay lại với PySpark Driver do đó không thể tái sử dụng và chia sẻ các biến này giữa các tasks.\n",
    "\n",
    "- Do đó PySpark sử dụng các <u>shared variables</u> được cached tại tất cả các nodes:\n",
    "    + **Broadcast variables (read-only shared variable)**: Việc sử dụng các broadcast varibles sẽ hạn chế việc phải gửi chúng mỗi lần gửi task, do đó tiết kiệm được chi phí communication.\n",
    "        * One of the best use-case of PySpark RDD Broadcast is to use with lookup data như zip code, state, country lookups e.t.c (các giá trị mặc định mà cần look up thường xuyên)\n",
    "        * The broadcasted data is cache in serialized format and deserialized before executing each task\n",
    "    + **Accumulator variables (updatable shared variables)**: là một shared variables khác nhưng cho phép add dữ liệu vào (không chỉ read-only, nhưng chỉ add)\n",
    "        * **named accumulators**: được hiển thị trên Job UI trong “Accumulator” tab. Trong tab có 2 tables: accumulable (chứa các tên biến và giá trị của chúng) và tasks (chứa value cho từng accumulator được modified bởi task)\n",
    "        * **unnamed accumulators**: không được lưu trữ, không truyền giá trị string khi khởi tạo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "broadcastVar = spark.sparkContext.broadcast([0, 1, 2, 3])\n",
    "print(broadcastVar.value)\n",
    "broadcastVar.unpersist()  # giải phóng khỏi cached ở tất cả executors. Khi cần dùng thì sẽ phải gửi lại cho từng executor\n",
    "broadcastVar.destroy()    # delete toàn bộ data and metadata. Không thể dùng nữa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một vài loại accumulators:\n",
    "- Long Accumulator: isZero, copy, reset, add, count, sum, avg, merge, value\n",
    "- Double Accumulator \n",
    "- Collection Accumulator: isZero, copyAndReset, copy, reset, add, merge, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "accum = spark.sparkContext.accumulator(value=1)\n",
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).foreach(lambda x: accum.add(x))\n",
    "\n",
    "print(accum.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
